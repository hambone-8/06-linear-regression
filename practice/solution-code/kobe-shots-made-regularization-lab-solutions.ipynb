{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Predicting shots made per game by Kobe Bryant\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab you'll be using regularized regression penalties Ridge, Lasso, and Elastic Net to try and predict how many shots Kobe Bryant made per game in his career.\n",
    "\n",
    "The Kobe shots dataset has hundreds of columns representing different characteristics of each basketball game. Fitting an ordinary linear regression using every predictor would dramatically overfit the model considering the limited number of observations (games) we have available. Furthermore, many of the predictors have significant multicollinearity. \n",
    "\n",
    "\n",
    "**Warning:** Some of these calculations are computationally expensive and may take a while to execute.  It may be worth while to only use a portion of the data to perform these calculations, especially if you have experienced kernel issues in the past.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "kobe = pd.read_csv('../datasets/kobe_superwide_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Examine the data\n",
    "\n",
    "- How many columns are there?\n",
    "- Examine what the observations (rows) and columns represent.\n",
    "- Why is this data that regularization might be particularly useful for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Columns:', len(kobe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(kobe.columns[0:20])\n",
    "\n",
    "# The columns are various statistics for each game. \n",
    "# There is a column SHOTS_MADE that will be our target variable for prediction\n",
    "# This is good for regularization because there are so many columns (feature selection)\n",
    "# and many of the columns represent similar things (multicollinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Make predictor and target variables. Standardize the predictors.\n",
    "\n",
    "Why is normalization necessary for regularized regressions?\n",
    "\n",
    "Use the `sklearn.preprocessing` class `StandardScaler` to standardize the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = kobe.iloc[:,1:]\n",
    "y = kobe.SHOTS_MADE.values\n",
    "\n",
    "# Initialize the StandardScaler object\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "# use the \"fit_transform\" function to standardize the X design matrix\n",
    "Xs = ss.fit_transform(X)\n",
    "\n",
    "# Standardization is necessary for regularized regression because the beta\n",
    "# values for each predictor variable must be on the same scale. If betas\n",
    "# are different sizes just because of the scale of predictor variables\n",
    "# the regularization term can't determine which betas are more/less \n",
    "# important based on their size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Build a linear regression predicting `SHOTS_MADE` from the rest of the columns.\n",
    "\n",
    "Cross-validate the $R^2$ of an ordinary linear regression model with 10 cross-validation folds.\n",
    "\n",
    "How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "\n",
    "linreg_scores = cross_val_score(linreg, Xs, y, cv=10)\n",
    "\n",
    "print(linreg_scores)\n",
    "print(np.mean(linreg_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean R^2 is extremely negative. All the R^2 scores are negative in crossvalidation.\n",
    "# The linear regression is performing far worse than baseline on the test sets.\n",
    "# It is probably dramatically overfitting and the redundant variables are affecting\n",
    "# the coefficients in weird ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Find an optimal value for Ridge regression alpha using `RidgeCV`.\n",
    "\n",
    "[Go to the documentation and read how RidgeCV works.](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "\n",
    "> *Hint: once the RidgeCV is fit, the attribute `.alpha_` contains the best alpha parameter it found through cross-validation.*\n",
    "\n",
    "Recall that Ridge performs best searching alphas through logarithmic space (`np.logspace`). This may take awhile to fit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ridge_alphas = np.logspace(0, 5, 200)\n",
    "\n",
    "optimal_ridge = RidgeCV(alphas=ridge_alphas, cv=10)\n",
    "optimal_ridge.fit(Xs, y)\n",
    "\n",
    "print(optimal_ridge.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Cross-validate the Ridge regression $R^2$ with the optimal alpha.\n",
    "\n",
    "Is it better than the Linear regression? If so, why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=optimal_ridge.alpha_)\n",
    "\n",
    "ridge_scores = cross_val_score(ridge, Xs, y, cv=10)\n",
    "\n",
    "print(ridge_scores)\n",
    "print(np.mean(ridge_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's vastly better than the Linear Regression. \n",
    "# There is likely so much multicollinearity in the data that \"vanilla\" regression\n",
    "# overfits and has bogus coefficients on predictors. Ridge is \n",
    "# able to manage the multicollinearity and get a good out-of-sample result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Find an optimal value for Lasso regression alpha using `LassoCV`.\n",
    "\n",
    "[Go to the documentation and read how LassoCV works.](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) It is very similar to `RidgeCV`.\n",
    "\n",
    "> *Hint: again, once the `LassoCV` is fit, the attribute `.alpha_` contains the best alpha parameter it found through cross-validation.*\n",
    "\n",
    "Recall that Lasso, unlike Ridge, performs best searching for alpha through linear space (`np.linspace`). However, you can actually let the LassoCV decide itself what alphas to use by instead setting the keyword argument `n_alphas=` to however many alphas you want it to search over. It is recommended to let sklearn choose the range of alphas.\n",
    "\n",
    "_**Tip:** If you find your CV taking a long time and you're not sure if its working set `verbose =1`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimal_lasso = LassoCV(n_alphas=500, cv=10, verbose=1)\n",
    "optimal_lasso.fit(Xs, y)\n",
    "\n",
    "print(optimal_lasso.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. Cross-validate the Lasso $R^2$ with the optimal alpha.\n",
    "\n",
    "Is it better than the Linear regression? Is it better than Ridge? What do the differences in results imply about the issues with the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=optimal_lasso.alpha_)\n",
    "\n",
    "lasso_scores = cross_val_score(lasso, Xs, y, cv=10)\n",
    "\n",
    "print(lasso_scores)\n",
    "print(np.mean(lasso_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lasso performs slightly better than the Ridge, but similarly.\n",
    "# Lasso deals primarily with the feature selection of valuable variables,\n",
    "# eliminating ones that are not useful. This also takes care of multicollinearity,\n",
    "# but in a different way: it will choose the \"best\" of the correlated variables\n",
    "# and zero-out the other redundant ones.\n",
    "# There may also be useless variables in the data which it is simply getting rid\n",
    "# of entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. Look at the coefficients for variables in the Lasso.\n",
    "\n",
    "1. Show the coefficient for variables, ordered from largest to smallest coefficient by absolute value.\n",
    "2. What percent of the variables in the original dataset are \"zeroed-out\" by the lasso?\n",
    "3. What are the most important predictors for how many shots Kobe made in a game?\n",
    "\n",
    "> **Note:** if you only fit the Lasso within `cross_val_score`, you will have to refit it outside of that\n",
    "function to pull out the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lasso.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lasso_coefs = pd.DataFrame({'variable':X.columns,\n",
    "                            'coef':lasso.coef_,\n",
    "                            'abs_coef':np.abs(lasso.coef_)})\n",
    "\n",
    "lasso_coefs.sort_values('abs_coef', inplace=True, ascending=False)\n",
    "\n",
    "lasso_coefs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Percent variables zeroed out:', np.sum((lasso.coef_ == 0))/float(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 10. Find an optimal value for Elastic Net regression alpha using `ElasticNetCV`.\n",
    "\n",
    "[Go to the documentation and read how LassoCV works.](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html).\n",
    "\n",
    "Note here that you will be optimizing both the alpha parameter and the l1_ratio:\n",
    "- `alpha`: strength of regularization\n",
    "- `l1_ratio`: amount of ridge vs. lasso (0 = all ridge, 1 = all lasso)\n",
    "    \n",
    "Do not include 0 in the search for `l1_ratio`: it will not allow it and break!\n",
    "\n",
    "You can use `n_alphas` for the alpha parameters instead of setting your own values: highly recommended!\n",
    "\n",
    "Also - be careful setting too many l1_ratios over cross-validation folds in your search. It can take a very long time if you choose too many combinations and for the most part there are diminishing returns in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "l1_ratios = np.linspace(0.01, 1.0, 25)\n",
    "\n",
    "optimal_enet = ElasticNetCV(l1_ratio=l1_ratios, n_alphas=100, cv=10,\n",
    "                            verbose=1)\n",
    "optimal_enet.fit(Xs, y)\n",
    "\n",
    "print(optimal_enet.alpha_)\n",
    "print(optimal_enet.l1_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11. Cross-validate the ElasticNet $R^2$ with the optimal alpha and l1_ratio.\n",
    "\n",
    "How does it compare to the Ridge and Lasso regularized regressions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "enet = ElasticNet(alpha=optimal_enet.alpha_, l1_ratio=optimal_enet.l1_ratio_)\n",
    "\n",
    "enet_scores = cross_val_score(enet, Xs, y, cv=10)\n",
    "\n",
    "print(enet_scores)\n",
    "print(np.mean(enet_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Performs basically the same as lasso, which is to be expected given the\n",
    "# l1_ration approaching 1 (full lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 12. [Bonus] Compare the residuals for the Ridge and Lasso visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Need to fit the ElasticNet and Ridge outside of cross_val_score like i did with the ridge\n",
    "ridge.fit(Xs, y)\n",
    "lasso.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# model residuals:\n",
    "\n",
    "ridge_resid = y - ridge.predict(Xs)\n",
    "lasso_resid = y - lasso.predict(Xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(ridge_resid, lasso_resid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
