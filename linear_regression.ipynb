{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Objectives\n",
    "- Define data modeling and simple linear regression.\n",
    "- Build a linear regression model using a data set that meets the linearity assumption using the scikit-learn library.\n",
    "- Understand and identify multicollinearity in a multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduce the Bikeshare Data Set](#introduce-the-bikeshare-dataset)\n",
    "\t- [Read in the  Capital Bikeshare Data](#read-in-the--capital-bikeshare-data)\n",
    "\t- [Visualizing the Data](#visualizing-the-data)\n",
    "- [Linear Regression Basics](#linear-regression-basics)\n",
    "\t- [Form of Linear Regression](#form-of-linear-regression)\n",
    "- [Overview of Supervised Learning](#overview-of-supervised-learning)\n",
    "\t- [Benefits and Drawbacks of scikit-learn](#benefits-and-drawbacks-of-scikit-learn)\n",
    "\t- [Requirements for Working With Data in scikit-learn](#requirements-for-working-with-data-in-scikit-learn)\n",
    "\t- [Building a Linear Regression Model in sklearn](#building-a-linear-regression-model-in-sklearn)\n",
    "\t- [scikit-learn's Four-Step Modeling Pattern](#scikit-learns--step-modeling-pattern)\n",
    "- [Build a Linear Regression Model](#build-a-linear-regression-model)\n",
    "- [Using the Model for Prediction](#using-the-model-for-prediction)\n",
    "\t- [Does the Scale of the Features Matter?](#does-the-scale-of-the-features-matter)\n",
    "- [Work With Multiple Features](#work-with-multiple-features)\n",
    "\t- [Visualizing the Data (Part 2)](#visualizing-the-data-part-)\n",
    "\t- [Adding More Features to the Model](#adding-more-features-to-the-model)\n",
    "- [What Is Multicollinearity?](#what-is-multicollinearity)\n",
    "- [How to Select a Model](#how-to-select-a-model)\n",
    "\t- [Feature Selection](#feature-selection)\n",
    "\t- [Evaluation Metrics for Regression Problems](#evaluation-metrics-for-regression-problems)\n",
    "\t- [Comparing Models With Train/Test Split and RMSE](#comparing-models-with-traintest-split-and-rmse)\n",
    "\t- [Comparing Testing RMSE With Null RMSE](#comparing-testing-rmse-with-null-rmse)\n",
    "- [Feature Engineering to Improve Performance](#feature-engineering-to-improve-performance)\n",
    "\t- [Handling Categorical Features](#handling-categorical-features)\n",
    "\t- [Feature Engineering](#feature-engineering)\n",
    "- [Bonus Material: Regularization](#bonus-material-regularization)\n",
    "\t- [How Does Regularization Work?](#how-does-regularization-work)\n",
    "\t- [Lasso and Ridge Path Diagrams](#lasso-and-ridge-path-diagrams)\n",
    "\t- [Advice for Applying Regularization](#advice-for-applying-regularization)\n",
    "\t- [Ridge Regression](#ridge-regression)\n",
    "- [Comparing Linear Regression With Other Models](#comparing-linear-regression-with-other-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Modeling?\n",
    "\n",
    "A Data Model for a Data Scientist is an artifact created by the machine learning process one might even consider a program in its own right. The model will accept data and return the appropriate output. As discussed before, in supervised learning a model is the combination of the algorithm trained with the training data.\n",
    "\n",
    "The overall process is fairly linear and is the end result of our Data Science process. Our goal is to:\n",
    "\n",
    "1. Create a Hypothesis or a question we want to test/explore\n",
    "\n",
    "2. Load, clean and transform relevant data\n",
    "\n",
    "3. Identify relevant features/variables for both the question and the model\n",
    "\n",
    "4. Build a process with an appropriate machine learning algorithm or statistical methodology that suits your data, use case, and available computational resources\n",
    "\n",
    "5. Test, evaluate and refine your model. Often, Data Scientists create numerous models until aligning on the one creating the best output\n",
    "\n",
    "6. Deploy the model - This can be accomplished through batch processing or moved into a real time production environment\n",
    "\n",
    "7. Monitor and refine the model over time\n",
    "\n",
    "\n",
    "The end result is a clear and defined process to accept raw information and create predictive or prescriptive insights to your organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduce-the-bikeshare-dataset\"></a>\n",
    "## Introduce the Bikeshare Data Set\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with a data set from Capital Bikeshare that was used in a Kaggle competition ([data dictionary](https://www.kaggle.com/c/bike-sharing-demand/data)).\n",
    "\n",
    "The objective of the competition is to **predict total ridership of Capital Bikeshare in any given hour.**\n",
    "\n",
    "Demand forecasting is a common data science application. If we can predict the quantity of demand, total ridership in a given hour, we can create analytical tools to improve the bikeshare system. \n",
    "Some applications would be:\n",
    "* Find where to site new bikeshare stations and know how large of a station to build.\n",
    "* Calculate the expected wear and tear on bikes and what the replacement costs will be.\n",
    "* Use a slightly different research design to forecast full and empty stations and send a service vehicle to \"rebalance\" the bikes from one station to another, as sometimes bikeshare stations have no bikes or are completely full and prevent use of the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:00:37.529086Z",
     "start_time": "2021-07-15T20:00:30.772795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specific imports\n",
    "# These are new! Notice we're using the 'from' approach to import only what we need.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Statistics imports\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# magic and parameters\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n",
    "### Read In the Capital Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:00:37.605923Z",
     "start_time": "2021-07-15T20:00:37.532069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data and set the datetime as the index.\n",
    "url = './data/bikeshare.csv'\n",
    "bikes_df = pd.read_csv(url, index_col='datetime', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used `index_col` to set an index or primary key for our data. In this case, the index of each row will be set to the value of its `datetime` field.\n",
    "\n",
    "We also ask Pandas to parse dates (if `parse_dates=True`, for the index only). So, rather than reading in a string, Pandas converts the index string to a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:00:37.632573Z",
     "start_time": "2021-07-15T20:00:37.609171Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preview the first five rows of the DataFrame.\n",
    "bikes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does each observation represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T00:23:59.347036Z",
     "start_time": "2020-12-10T00:23:59.342221Z"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the response variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T00:23:59.352900Z",
     "start_time": "2020-12-10T00:23:59.349697Z"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many features are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T00:23:59.359777Z",
     "start_time": "2020-12-10T00:23:59.356412Z"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable| Description |\n",
    "|---------|----------------|\n",
    "|datetime| hourly date + timestamp  |\n",
    "|season|  1=winter, 2=spring, 3=summer, 4=fall |\n",
    "|holiday| whether the day is considered a holiday|\n",
    "|workingday| whether the day is neither a weekend nor holiday|\n",
    "|weather| See Below|\n",
    "|temp| temperature in Celsius|\n",
    "|atemp| \"feels like\" temperature in Celsius|\n",
    "|humidity| relative humidity|\n",
    "|windspeed| wind speed|\n",
    "|casual| number of non-registered user rentals initiated|\n",
    "|registered| number of registered user rentals initiated|\n",
    "|count| number of total rentals|\n",
    "\n",
    "> _Details on Weather Variable_\n",
    "\n",
    "> **1**: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\n",
    "> **2**: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\n",
    "> **3**: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\n",
    "> **4**: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"count\" is a method in Pandas (and a very non-specific name), so it's best to name that column something else\n",
    "\n",
    "In general, you may want to rename columns if it is not obvious what might be stored in them. Although we will only rename the target column here, a few examples might be to rename:\n",
    "\n",
    "| old name | new name |\n",
    "| ---    | --- |\n",
    "| temp | temp_celcius\n",
    "| windspeed | windspeed_knots\n",
    "| casual | num_casual_users\n",
    "| registered | num_registered_users\n",
    "| season | season_num\n",
    "| holiday | is_holiday\n",
    "| workingday | is_workingday\n",
    "| humidity | humidity_percent\n",
    "\n",
    "Without having to check, these new names make it obvious what is stored in each column. The downside is slightly longer column names, which could affect table readability in Jupyter. It would be ideal to use very specific names in CSV files to assist others reading them. In your own code, use whatever makes sense for your work -- if you are viewing lots of Pandas tables, you may want to use shorter names. However, readable specific names are preferred in Python code since it helps prevent mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview-of-supervised-learning\"></a>\n",
    "## Overview of Supervised Learning\n",
    "---\n",
    "Supervised machine learning means the algorithm learns from labeled data.\n",
    "\n",
    "![Supervised learning diagram](./assets/supervised-learning.png) \n",
    "source: https://blogs.nvidia.com/blog/2018/08/02/supervised-unsupervised-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:36:31.853355Z",
     "start_time": "2021-07-15T20:36:31.744171Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the .rename() method to rename count to total\n",
    "bikes_df.rename(columns={'count':'total_rentals'}, inplace=True)\n",
    "bikes_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data\"></a>\n",
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have a general feeling for what the data looks like before building a model. Ideally, before creating the model you would have some sense of which variables might matter most to predict the response. This dataset is fairly intuitive (and the purpose of this lesson is not visualization), so we will keep the visualization short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View the distribution of total rentals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:38:24.872132Z",
     "start_time": "2021-07-15T20:38:23.811727Z"
    }
   },
   "outputs": [],
   "source": [
    "# What method should we use to visualize this data?\n",
    "bikes_df['total_rentals'].plot()  # what goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View variable correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:39:26.338437Z",
     "start_time": "2021-07-15T20:39:25.857256Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(bikes_df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable relationships**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:40:36.848640Z",
     "start_time": "2021-07-15T20:40:36.357321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas scatterplot\n",
    "bikes_df.plot(kind='scatter', x='registered', y='total_rentals', alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classroom Discussion\n",
    "Is using `registered` as a variable a good choice in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:45:45.725402Z",
     "start_time": "2021-07-15T20:45:45.167071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas scatterplot\n",
    "bikes_df.plot(kind='scatter', x='temp', y='total_rentals', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T20:47:11.162546Z",
     "start_time": "2021-07-15T20:47:08.754089Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Seaborn scatterplot with regression line\n",
    "sns.lmplot(x='temp', y='total_rentals', data=bikes_df, aspect=1.5, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-basics\"></a>\n",
    "## Linear Regression Basics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When it works best\n",
    "\n",
    "- Data is normally distributed (but doesn't have to be)\n",
    "- X's significantly explain y (low p-values)\n",
    "- X's are independent of each other (little to no multicollinearity)\n",
    "- Resulting values pass a linear assumption \n",
    "\n",
    "The regression has six key assumptions:\n",
    "\n",
    "1. Linear relationship between target and features\n",
    "2. Data is normally distributed or contains Multivariate normality (but doesn't have to be)\n",
    "3. No or little multicollinearity\n",
    "4. No auto-correlation\n",
    "5. Homoscedasticity\n",
    "6. Independent features\n",
    "\n",
    "Note: If data is not normally distributed, we could be introducing bias\n",
    "\n",
    "**1. Linear Relationship** \n",
    "\n",
    "First, linear regression needs the relationship between the independent and dependent variables to be linear.  It is also important to check for outliers since linear regression is sensitive to outlier effects.  The linearity assumption can best be tested with scatter plots\n",
    "\n",
    "**2. Normal Distribution**\n",
    "\n",
    "Since the linear regression analysis requires all variables to be multivariate normal it can be checked with a histogram or Q-Q plot. Normality can also be checked with a goodness of fit test like the [Kolmogorov-Smirnov test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.htmll) . When the data is not normally distributed it's possible a non-linear transformation (e.g., log-transformation) might fix this issue.\n",
    "\n",
    "**3. Low to No Multicollinearity**\n",
    "\n",
    "Multicollinearity occurs when the independent variables are too highly correlated with each other. A quick way to test this is our tried and true correlation matrix. \n",
    "\n",
    "If multicollinearity is found in the data centering the data, that is deducting the mean score might help to solve the problem.  Other alternatives to tackle the problems is conducting a factor analysis and rotating the factors to insure independence of the factors in the linear regression analysis.\n",
    "\n",
    "**4. Low to No Auto-correlation**\n",
    "\n",
    "Linear regression analysis requires that there is little or no autocorrelation in the data.  Autocorrelation occurs when the residuals are not independent from each other.  In other words when the value of y(x+1) is not independent from the value of y(x). Examples are signal processing or time series data\n",
    "\n",
    "While a scatterplot allows you to check for autocorrelations, you can test the linear regression model for autocorrelation with the [Durbin-Watson test](http://www.statsmodels.org/dev/generated/statsmodels.stats.stattools.durbin_watson.html).  Durbin-Watson’s d tests the null hypothesis that the residuals are not linearly auto-correlated.  While d can assume values between 0 and 4, values around 2 indicate no autocorrelation.  As a rule of thumb values of 1.5 < d < 2.5 show that there is no auto-correlation in the data. However, the Durbin-Watson test only analyses linear autocorrelation and only between direct neighbors, which are first order effects.\n",
    "\n",
    "\n",
    "**5. Homoscedasticity**\n",
    "\n",
    "Homoscedastic data means the residuals are equal across the regression line. We can test this with scatterplots or seaborn's lmplot()\n",
    "\n",
    "**6. Independent features**\n",
    "\n",
    "Independent features are in no way derived from other features. Imagine a dataset composed of three features. The first two features are in no way related but the third feature is simply the sum of the first two. That means this ficitonal dataset has one linearly dependent feature. That’s a problem for linear regression since we expect to interpret our outputs as X increase in feature A drives an increase of Y units of the response variable. However, if features are correlated, you lose the ability to interpret the linear regression model because you violate a fundamental assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"form-of-linear-regression\"></a>\n",
    "### Form of Linear Regression\n",
    "\n",
    "Recall that each model always contains some amount of random irreducible error $\\epsilon$. So, given a prediction $\\hat{y}$, the actual $y = \\hat{y} + \\epsilon$. Below, we will assume $y$ is exactly linear.\n",
    "\n",
    "- We're all familiar with the formula for a line: $\\hat{y} = mx + b$.\n",
    "- In the literature for machine learning it's often written as follows: $\\hat{y} = \\theta_0 + \\theta_1 x$. In this formulation $\\theta_1$ is called a weight instead of a coefficient and $\\theta_0$ is called a bias instead of intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize this to $n$ independent variables as follows:\n",
    "\n",
    "$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n + \\epsilon$\n",
    "\n",
    "- $y$ is the response.\n",
    "- $\\theta_0$ is the intercept.\n",
    "- $\\theta_1$ is the coefficient for $x_1$ (the first feature).\n",
    "- $\\theta_n$ is the coefficient for $x_n$ (the nth feature).\n",
    "- $\\epsilon$ is the _error_ term\n",
    "\n",
    "An example of this applied to our bikeshare data might be:\n",
    "\n",
    "$\\text{total_rides} = 20 + -2 \\cdot \\text{temp} + -3 \\cdot \\text{windspeed}\\ +\\ ...\\ +\\ 0.1 \\cdot \\text{registered}$\n",
    "\n",
    "This equation is still called **linear** because the highest degree of the independent variables (e.g. $x_i$) is 1. Note that because the $\\theta$ values are constants, they will not be independent variables in the final model, as seen above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\theta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are trying to find the line (mathematically) that minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- Once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](./assets/estimating_coefficients.png)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Linear Regression in Python\n",
    "Before we jump into building a model with sci-kit learn let's look under the hood of how linear regressions are generated using gradient descent (*i.e.* a least squares approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best line\n",
    "\n",
    "We need a measure to help determine the best line. The measure we use is known as the **cost function**, which is related to the error. The error is the difference between the actual values and those that lie along the generated line. The best fit line is the one that minimizes the cost function.\n",
    "\n",
    "##### The Cost Function\n",
    "\n",
    "$$ \\large{J(\\theta)} = \\frac{1}{2n} \\Sigma_{i}^{n} (\\hat{y}_{\\theta}(x^1_i,x^2_i,x^3_i) - y_i)^2  $$\n",
    "\n",
    "where $\\hat{y}_{\\theta}$ is the linear model given by:\n",
    "\n",
    "$$\\large \\hat{y}_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_m x_m$$\n",
    "\n",
    "This is in the form $y = mx + b $. So when $x$ is 1-dimensional $\\theta_1$ is the slope and $\\theta_0$ is the y-intercept.\n",
    "\n",
    "We use a step-by-step process called gradient descent to find the line that provides the minimum cost function, which also corresponds to the least total error. \n",
    "\n",
    "#### Gradient Descent\n",
    "We use gradient descent to determine how to change our regression line. The cost function is a multivariable function $J(\\theta_1, \\theta_0)$ and gradient descent of $J(\\theta)$ shows how changes in the $\\theta_0$ and the $\\theta_1$ changes the value of $J(\\theta)$. We use these results to find minimum values for the cost function.    \n",
    "\n",
    "If you remember your vector calculus, you'll recognize the gradient of a function as the partial derivatives with respect to each of the variables, so:\n",
    "\n",
    "$$\\large \\nabla J(\\theta_1, \\theta_0) = \\frac{\\delta J}{\\delta \\theta_1} , \\frac{\\delta J}{\\delta \\theta_0}$$\n",
    "\n",
    "Without going through all the math we'll end up here, which is something we can code in Python. Note we update $\\theta_j$ for all $j$.\n",
    "\n",
    "$$ \\large \\theta_{j} := \\theta_j - \\alpha \\frac{1}{n} \\Sigma_i^n (\\hat{y}_{\\theta}(x_i) - y_i)x_i $$\n",
    "\n",
    "Note we have added a constant $\\alpha$ which is known as the learning rate. The learning rate  ensures the steps we take aren't too large or too small. There is always a tradeoff: if the learning rate is too large we might overshoot our minimum, if it is too small it may take too long to find the minimum.\n",
    "\n",
    "For the case where $\\hat{y}_{\\theta}(x) = \\theta_0 + \\theta_1 x$ we have:\n",
    "\n",
    "$$ \\large \\theta_{0} := \\theta_0 - \\alpha \\frac{1}{n} \\Sigma_i^n (\\hat{y}_{\\theta}(x_i) - y_i) $$\n",
    "\n",
    "$$ \\large \\theta_{1} := \\theta_1 - \\alpha \\frac{1}{n} \\Sigma_i^n (\\hat{y}_{\\theta}(x_i) - y_i)x_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:20:13.812715Z",
     "start_time": "2021-07-15T21:20:13.244319Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a fake data set to demonstrate linear regression\n",
    "np.random.seed(225) # set a random seed\n",
    "\n",
    "# create the X data\n",
    "X = np.random.rand(100,1).reshape(100)\n",
    "\n",
    "# create random noise\n",
    "noise = np.random.normal(0,3,100)\n",
    "\n",
    "# create y values and add random noise\n",
    "y = 3 + 50* X + noise\n",
    "\n",
    "# generate a scatter plot of the data\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(X, y, '.', c='blue')\n",
    "#plt.plot(x, regression_formula(x), '-', color=\"red\")\n",
    "plt.xlabel(\"X\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "plt.title('Fake Linear Regression Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:20:53.853423Z",
     "start_time": "2021-07-15T21:20:53.821463Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we have a Python function to compute the cost function and another for the gradient descent.\n",
    "\n",
    "def cost_function(X, y, theta):\n",
    "    \"\"\" \n",
    "        Compute the cost function (J) based on X, y and theta. J has the following form:\n",
    "        \n",
    "        J = 1/2n * Sum( (y_hat - y)^2 )\n",
    "    \n",
    "    Args:\n",
    "        X: array of values for the independent variable\n",
    "        y: array of values for the dependent variable\n",
    "        theta: array consisting of the coefficient theta_1 and intercept (or bias) theta_0\n",
    "        \n",
    "    Returns:\n",
    "        J: value of the cost function for the data (X,y) and a given theta\n",
    "    \"\"\"\n",
    "    # set up our cost function\n",
    "    n = len(y)\n",
    "    y_hat = X.dot(theta) # our linear equation \n",
    "    \n",
    "    # The cost function\n",
    "    J = 1/(2*n) * (np.sum( (y_hat-y)**2) )\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:21:20.801853Z",
     "start_time": "2021-07-15T21:21:20.790640Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Function to carry out gradient descent starting with initial values for coefficients and bias (theta).\n",
    "    \n",
    "    Args:\n",
    "        X: array of values for independent variable\n",
    "        y: array of values for the dependent variable\n",
    "        theta: array of values for coefficients and bias\n",
    "        alpha: learning rate\n",
    "        iterations: integer defining how many steps the process should take\n",
    "        \n",
    "    Returns:\n",
    "        theta: updated value for theta\n",
    "        J_old: array of values for cost function computed at each step.\n",
    "    This function takes in the training data, the intial theta values(coefficients), the learning rate, \n",
    "    and the number of iterations. The output will be the a new set of coefficeient of the linear regression (theta),\n",
    "    optimized for making predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # empty arrays to hold the cost function values and slopes (theta_1).\n",
    "    J_array = []\n",
    "    theta_array = []\n",
    "    \n",
    "    n = len(y) # get the length of the array\n",
    "    \n",
    "    # loop to compute the new theta and cost function for each iteration\n",
    "    # Note we keep all the \n",
    "    for i in range(iterations): \n",
    "        # the linear equation\n",
    "        y_hat = X.dot(theta)\n",
    "        # the gradient descent operation\n",
    "        theta = theta - (alpha/n)*(X.T.dot(y_hat-y))\n",
    "        # append the cost\n",
    "        J_array.append(cost_function(X, y, theta))\n",
    "        # append\n",
    "        theta_array.append(theta[1])\n",
    "        \n",
    "    return theta, J_array, theta_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:21:52.125265Z",
     "start_time": "2021-07-15T21:21:52.114722Z"
    }
   },
   "outputs": [],
   "source": [
    "# some house keeping to put X and y in the proper form.\n",
    "\n",
    "# get the length of the y array\n",
    "n = len(y)\n",
    "\n",
    "# Append the intercept term (bias) to X and reshape X to a mx1 matrix\n",
    "X1 = np.append(np.ones([n,1]), X.reshape(n,1), axis=1)\n",
    "\n",
    "# Reshape y to n x1 matrix\n",
    "y1 = y.reshape(n,1)\n",
    "\n",
    "# Set initial coefficient to zero\n",
    "theta = np.zeros([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:21:57.360693Z",
     "start_time": "2021-07-15T21:21:57.351230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the cost\n",
    "cost = cost_function(X1, y1, theta)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:22:06.199302Z",
     "start_time": "2021-07-15T21:22:05.893670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the iteration parameter. Repeat this with 5000 iterations\n",
    "iterations = 5000 \n",
    "\n",
    "# Initialize the learning rate.\n",
    "alpha = 0.01\n",
    "\n",
    "## Call the function and pass in the parameters to compute new coefficient values.\n",
    "new_theta, J_array, theta_array = gradient_descent(X1, y1, theta, alpha, iterations) \n",
    "new_cost = cost_function(X1, y1, new_theta)\n",
    "\n",
    "print(f'Cost = {cost}')\n",
    "print(f'theta_0 = {new_theta[0][0]}')\n",
    "print(f'theta_1 = {new_theta[1][0]}')\n",
    "print(f'New Cost = {new_cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:26:43.247746Z",
     "start_time": "2021-07-15T21:26:42.812832Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def regression formula\n",
    "\n",
    "f = lambda x: new_theta[0] + new_theta[1] * x\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(X, y, '.', c='blue')\n",
    "plt.plot(X, f(X), '-', color=\"red\")\n",
    "plt.xlabel(\"X\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);\n",
    "plt.title('Fake Linear Regression Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:26:57.947936Z",
     "start_time": "2021-07-15T21:26:57.591709Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(theta_array, J_array, '.', c='blue')\n",
    "#plt.plot(X, f(X), '-', color=\"red\")\n",
    "plt.xlabel(\"theta (slope)\", fontsize=14)\n",
    "plt.ylabel(\"Cost\", fontsize=14);\n",
    "plt.title('Cost as function of slope');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example using the Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:29:29.903004Z",
     "start_time": "2021-07-15T21:29:29.890316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define X & y\n",
    "X = bikes_df['temp'].values\n",
    "y = bikes_df['total_rentals'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:29:43.385104Z",
     "start_time": "2021-07-15T21:29:43.367116Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape the arrays\n",
    "n = len(y)\n",
    "\n",
    "# Append the intercept term (bias) to X and reshape X to a mx1 matrix\n",
    "X1 = np.append(np.ones([n,1]), X.reshape(n,1), axis=1)\n",
    "\n",
    "# Reshape y to n x1 matrix\n",
    "y1 = y.reshape(n,1)\n",
    "\n",
    "# Set initial coefficient to zero\n",
    "theta = np.zeros([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:30:07.025673Z",
     "start_time": "2021-07-15T21:29:54.484322Z"
    }
   },
   "outputs": [],
   "source": [
    "cost = cost_function(X1, y1, theta)#Call the function and pass in values for X, y, and theta to compute the cost.\n",
    "print(f'Cost = {cost}')\n",
    "\n",
    "iterations = 50000 # try 50000 later\n",
    "alpha = 0.001\n",
    "## Call the function and pass in the parameters to compute new coefficient values.\n",
    "new_theta, J_array, theta_array = gradient_descent(X1, y1, theta, alpha, iterations) \n",
    "\n",
    "print(f'theta_0 = {new_theta[0][0]}')\n",
    "print(f'theta_1 = {new_theta[1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:30:07.877781Z",
     "start_time": "2021-07-15T21:30:07.351220Z"
    }
   },
   "outputs": [],
   "source": [
    "# def regression formula\n",
    "\n",
    "f = lambda x: new_theta[0] + new_theta[1] * x\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(X, y, '.', alpha=0.2)\n",
    "plt.plot(X, f(X), '-', color=\"red\")\n",
    "plt.xlabel(\"temp\", fontsize=16)\n",
    "plt.ylabel(\"total rentals\", fontsize=16);\n",
    "plt.title('Bike Share Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"benefits-and-drawbacks-of-scikit-learn\"></a>\n",
    "### Benefits and Drawbacks of scikit-learn\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Consistent interface to machine learning models.\n",
    "- Provides many tuning parameters but with sensible defaults.\n",
    "- Exceptional documentation.\n",
    "- Rich set of functionality for companion tasks.\n",
    "- Active community for development and support.\n",
    "\n",
    "**Potential drawbacks:**\n",
    "\n",
    "- Harder (than R) to get started with machine learning.\n",
    "- Less emphasis (than R) on model interpretability.\n",
    "    - scikit-learn tends not to run detailed statistical tests, e.g. ANOVA.\n",
    "    - For more detail on model fit, try the `statsmodels` library.\n",
    "\n",
    "Ben Lorica: [Six Reasons Why I Recommend scikit-learn](http://radar.oreilly.com/2013/12/six-reasons-why-i-recommend-scikit-learn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"requirements-for-working-with-data-in-scikit-learn\"></a>\n",
    "### Requirements for Working With Data in scikit-learn\n",
    "\n",
    "1. Features and response should be separate objects.\n",
    "2. Features and response should be entirely numeric.\n",
    "3. Features and response should be NumPy arrays (or easily converted to NumPy arrays).\n",
    "4. Features and response should have specific shapes (outlined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building-a-linear-regression-model-in-sklearn\"></a>\n",
    "### Building a Linear Regression Model in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a feature matrix called X that holds a `DataFrame` with only the temp variable and a `Series` called y that has the \"total_rentals\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:37:59.950243Z",
     "start_time": "2021-07-15T21:37:59.944687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "feature_cols = ['temp']\n",
    "X = bikes_df[feature_cols]\n",
    "y = bikes_df['total_rentals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:36:54.993097Z",
     "start_time": "2021-07-15T21:36:54.986735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check X's type.\n",
    "print((type(X)))\n",
    "print((type(X.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:36:56.669448Z",
     "start_time": "2021-07-15T21:36:56.665731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check y's type.\n",
    "print((type(y)))\n",
    "print((type(y.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:38:05.552264Z",
     "start_time": "2021-07-15T21:38:05.548857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check X's shape (n = number of observations, p = number of features).\n",
    "print((X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:38:08.473255Z",
     "start_time": "2021-07-15T21:38:08.454523Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check y's shape (single dimension with length n).\n",
    "# The comma indicates the datatype is a tuple.\n",
    "print((y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scikit-learns--step-modeling-pattern\"></a>\n",
    "### scikit-learn's Four-Step Modeling Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import the class you plan to use. <br> \n",
    "**Step 2:** \"Instantiate\" the \"estimator.\" <br>\n",
    "**Step 3:** Fit the model with data (aka \"model training\"). <br>\n",
    "**Step 4:** Predict the response for a new observation. <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Step 1:** **Import the class/module/method you plan to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T07:02:56.020697Z",
     "start_time": "2020-12-07T07:02:56.014874Z"
    }
   },
   "outputs": [],
   "source": [
    "# You will normally do all your imports at the top of the notebook. \n",
    "# This is here to introduce the correct module in the context of the lesson.\n",
    "\n",
    "#from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** **\"Instantiate\" the \"estimator.\"**\n",
    "\n",
    "- \"Estimator\" is scikit-learn's term for \"model.\"\n",
    "- \"Instantiate\" means \"make an instance of.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:40:02.273830Z",
     "start_time": "2021-07-15T21:40:02.247972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make an instance of a LinearRegression object.\n",
    "# Instantiate the LR Object\n",
    "lr = LinearRegression()\n",
    "type(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created an object that \"knows\" how to do linear regression, and is just waiting for data.\n",
    "- Name of the object does not matter.\n",
    "- All parameters not specified are set to their defaults.\n",
    "- Can specify tuning parameters (aka \"hyperparameters\") during this step. \n",
    "\n",
    "To view the possible parameters, either use the `help` built-in function or evaluate the newly instantiated model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T07:02:57.596750Z",
     "start_time": "2020-12-07T07:02:57.594790Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** **Fit the model with data (aka \"model training\").**\n",
    "\n",
    "- Model is \"learning\" the relationship between X and y in our \"training data.\"\n",
    "- Process through which learning occurs varies by model.\n",
    "- Occurs in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:41:45.645439Z",
     "start_time": "2021-07-15T21:41:45.531985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:42:36.895289Z",
     "start_time": "2021-07-15T21:42:36.873316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the coefficients - Why is this a list?\n",
    "print(f'coefficients: {lr.coef_}')\n",
    "\n",
    "# Print the intercept.\n",
    "print(f'intercept: {lr.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the intercept ($\\theta_0$):\n",
    "\n",
    "- It is the value of $y$ when all independent variables are 0.\n",
    "- Here, it is the estimated number of rentals when the temperature is 0 degrees Celsius.\n",
    "- **Note:** It does not always make sense to interpret the intercept. (Why?)\n",
    "\n",
    "Interpreting the \"temp\" coefficient ($\\theta_1$):\n",
    "\n",
    "- **Interpretation:** An increase of 1 degree Celcius is _associated with_ increasing the number of total rentals by $\\theta_1$.\n",
    "- Here, a temperature increase of 1 degree Celsius is _associated with_ a rental increase of 9.17 bikes.\n",
    "- This is not a statement of causation.\n",
    "- $\\theta_1$ would be **negative** if an increase in temperature was associated with a **decrease** in total rentals.\n",
    "- $\\theta_1$ would be **zero** if temperature is not associated with total rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:43:55.436201Z",
     "start_time": "2021-07-15T21:43:55.407452Z"
    }
   },
   "outputs": [],
   "source": [
    "dict(zip(X.columns, lr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once a model has been fit with data, it's called a \"fitted model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Predict the response for a new observation.\n",
    "\n",
    "- New observations are called \"out-of-sample\" data.\n",
    "- Uses the information it learned during the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:45:55.106696Z",
     "start_time": "2021-07-15T21:45:55.080140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Per future warning, one-dimensional arrays must be reshaped using the following.\n",
    "lr.predict( np.array([0]).reshape(1,-1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model to make two predictions, one when the `temp` is 0 and another when the `temp` is 10. To do this, our feature matrix is always a 2-D array where each row is a list of features. Since we only have a single feature, the temperature, each row will contain only a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:48:44.093815Z",
     "start_time": "2021-07-15T21:48:44.083006Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = [[0],[10],[20], [200]]\n",
    "lr.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns a NumPy array, and we keep track of what the numbers \"mean.\"\n",
    "- Can predict for multiple observations at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just predicted using our model is, \"If the temperature is 0 degrees, the total number of bike rentals will be ~6.046, and if the temperature is 10 degrees the total number of bike rentals will ~97.751.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Format for sklearn model classes & methods\n",
    "\n",
    "#### Generate an instance of an estimator class\n",
    "estimator = base_models.AnySKLearnObject()\n",
    "#### Fit your data\n",
    "estimator.fit(X, y)\n",
    "#### Score it with the default scoring method (recommended to use the metrics module in the future)\n",
    "estimator.score(X, y)\n",
    "#### Predict a new set of data\n",
    "estimator.predict(new_X)\n",
    "#### Transform a new X if changes were made to the original X while fitting\n",
    "estimator.transform(new_X)\n",
    "\n",
    "- LinearRegression() doesn’t have a transform function\n",
    "\n",
    "- With this information, we can build a simple process for linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build-a-linear-regression-model\"></a>\n",
    "### Exercises\n",
    "\n",
    "#### 1. Las Vegas Simple Model\n",
    "We will use a [dataset](https://archive.ics.uci.edu/ml/datasets/Las+Vegas+Strip) of Tripadvisor reviews of Las Vegas hotels from 2015 to build a regression model predicting the customers score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T21:53:06.709251Z",
     "start_time": "2021-07-15T21:53:06.636562Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data into a dataframe named lv_reviews\n",
    "path = './data/LasVegasTripAdvisorReviews-Dataset.csv'\n",
    "\n",
    "lv_reviews = pd.read_csv('./data/LasVegasTripAdvisorReviews-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T07:04:41.555233Z",
     "start_time": "2020-12-07T07:04:41.526424Z"
    }
   },
   "outputs": [],
   "source": [
    "# display the first 2 rows of lv_reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a correlation matrix and clustermap of the variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T07:04:42.912189Z",
     "start_time": "2020-12-07T07:04:42.477008Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the distribution of the Score variable using a histogram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T07:04:43.850508Z",
     "start_time": "2020-12-07T07:04:43.642342Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create X and y using Hotel stars as the X variable and Score as the y variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:01:45.394579Z",
     "start_time": "2021-07-15T22:01:45.328672Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = ['Hotel stars']\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and fit a `LinearRegression` model on X and y from the `linear_model` section of scikit-learn.\n",
    "Be sure to use the scikit-learn \"recipe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:02:03.802889Z",
     "start_time": "2021-07-15T22:02:03.727568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit a linear regression\n",
    "# Import, instantiate, fit.\n",
    "#from sklearn.linear_model import LinearRegression # already imported\n",
    "\n",
    "linreg = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the coefficients and intercept.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T19:37:59.235248Z",
     "start_time": "2021-12-08T19:37:59.231828Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the coefficients and intercept. In your printed output be sure to identify them appropriate\n",
    "\n",
    "# print(linreg.intercept_)\n",
    "# print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpret the coefficients and intercept.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the intercept ($\\theta_0$):\n",
    "\n",
    "- It is the value of $y$ when all independent variables are 0.\n",
    "- The estimated score when the hotel has 0 stars.\n",
    "- **Note:** This is a case where the intercept would not be sensible, but it is still important to score observations.\n",
    "\n",
    "Interpreting the \"Hotel stars\" coefficient ($\\theta_1$):\n",
    "\n",
    "- **Interpretation:** An increase of 1 star is _associated with_ increasing the user score by $\\theta_1$.\n",
    "- Here, a increase of 1 star is _associated with_ a predicted increase of 0.187 in the score.\n",
    "- This is not a statement of causation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-the-model-for-prediction\"></a>\n",
    "## Using the Model for Prediction - Back to the Bike Data\n",
    "---\n",
    "\n",
    "While plenty of insight can be found in reading coefficients, the most common uses of data science focus on prediction. In scikit-learn we can make predictions from a fitted model using `.predict()`, but we will also go through the calculation by hand to understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many bike rentals would we predict if the temperature was 25 degrees Celsius?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the intercept and coefficients of the linear model.\n",
    "\n",
    "You can search for \"sklearn linear regression\" and explore the attributes section of the documentation to learn how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:03:55.551321Z",
     "start_time": "2021-07-15T22:03:55.526679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manually calculate the prediction.\n",
    "lr.intercept_+lr.coef_*25\n",
    "#6.04621295961681+9.17054048*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:04:02.240875Z",
     "start_time": "2021-07-15T22:04:02.236799Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lr.intercept_)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:04:37.983296Z",
     "start_time": "2021-07-15T22:04:37.969320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the predict method.\n",
    "lr.predict([ [0],[1],[25] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"does-the-scale-of-the-features-matter\"></a>\n",
    "### Does the Scale of the Features Matter?\n",
    "\n",
    "Let's say that temperature was measured in Fahrenheit, rather than Celsius. How would that affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:06:21.249814Z",
     "start_time": "2021-07-15T22:06:20.084295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column for Fahrenheit temperature.\n",
    "bikes_df['temp_F'] = bikes_df['temp'] * 9/5 + 32\n",
    "bikes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:06:43.348300Z",
     "start_time": "2021-07-15T22:06:41.672538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seaborn scatterplot with regression line\n",
    "sns.lmplot(x='temp_F', y='total_rentals', data=bikes_df, aspect=1.5, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild the `LinearRegression` from above using the `temp_F` features instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:07:29.579565Z",
     "start_time": "2021-07-15T22:07:29.561054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "feature_cols = ['temp_F']\n",
    "X = bikes_df[feature_cols]\n",
    "y = bikes_df.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert 25 degrees Celsius to Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:08:42.449793Z",
     "start_time": "2021-07-15T22:08:42.438063Z"
    }
   },
   "outputs": [],
   "source": [
    "# A function to convert Celsius to Fahrenheit temperature scale\n",
    "\n",
    "def celsius_fahrenheit(temp, cflag=True):\n",
    "    \"\"\"convert temperature from celsius to fahrenheit or fahrenheit to celsius\n",
    "\n",
    "        Uses the formula:\n",
    "                    F = C*1.8 + 32\n",
    "                    C = (F-32)/1.8\n",
    "                    \n",
    "    Args: temp: temperature in celsius or fahrenheit as an int or float\n",
    "          cflag: boolean, if true convert C-F, if false convert F-C\n",
    "    \n",
    "    Returns: converted temperature\"\"\"\n",
    "    \n",
    "    if cflag:\n",
    "        new_temp = temp*1.8 + 32\n",
    "    else:\n",
    "        new_temp = (temp - 32)/1.8\n",
    "        \n",
    "    return new_temp\n",
    "\n",
    "assert celsius_fahrenheit(77, False) == 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:08:45.684844Z",
     "start_time": "2021-07-15T22:08:45.637170Z"
    }
   },
   "outputs": [],
   "source": [
    "F = celsius_fahrenheit(25)\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict rentals for 77 degrees Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:09:08.236198Z",
     "start_time": "2021-07-15T22:09:08.227294Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'77 Degrees Fahrenheit Prediction {linreg.predict([[77]])} bikes rented.')\n",
    "print(f'25 Degrees Celsius Prediction {lr.predict([[25]])} bikes rented')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The scale of the features is irrelevant for linear regression models. When changing the scale, we simply change our interpretation of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:10:07.505077Z",
     "start_time": "2021-07-15T22:10:07.472432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the temp_F column.\n",
    "bikes_df.drop('temp_F', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"work-with-multiple-features\"></a>\n",
    "## Work With Multiple Features\n",
    "---\n",
    "\n",
    "We've demonstrated simple linear regression with one feature to gain an intuition, but the benefit of modeling is the ability to reason about hundreds of features at once. There is no limit to the number of features you can use. However, often a small set of features accounts for most of the variance (assuming there is a linear relationship at all). We will start by using four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data-part-\"></a>\n",
    "### Visualizing the Data (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:10:33.606148Z",
     "start_time": "2021-07-15T22:10:33.602677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create feature column variables\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a subset of scatterplot matrix using Seaborn.\n",
    "We can use pairplot with the y_vars argument to only show relationships with the `total_rentals` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:10:41.545135Z",
     "start_time": "2021-07-15T22:10:37.415200Z"
    }
   },
   "outputs": [],
   "source": [
    "# multiple scatterplots in Seaborn\n",
    "sns.pairplot(bikes_df, x_vars=feature_cols, y_vars='total_rentals', kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:13:51.119426Z",
     "start_time": "2021-07-15T22:13:40.888916Z"
    }
   },
   "outputs": [],
   "source": [
    "# alternative way in Pandas (might take a while)\n",
    "# scatter_matrix does a pairplot of *every* column\n",
    "\n",
    "grr = pd.plotting.scatter_matrix(bikes_df[['total_rentals'] + feature_cols], figsize=(15, 15), alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are you seeing anything you didn't expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the season variable using a cross-tab.\n",
    "**From the docstring**: \"Compute a simple cross tabulation of two (or more) factors. By default\n",
    "computes a frequency table of the factors unless an array of values and an\n",
    "aggregation function are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:13:51.624513Z",
     "start_time": "2021-07-15T22:13:51.123925Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-tabulation of season and month\n",
    "pd.crosstab(bikes_df['season'], bikes_df.index.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the season variable using a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:16:28.292693Z",
     "start_time": "2021-07-15T22:16:27.860468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Box plot of rentals, grouped by season\n",
    "bikes_df.boxplot(column='total_rentals', by='season');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at rentals over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:16:35.026848Z",
     "start_time": "2021-07-15T22:16:34.459087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Line plot of rentals\n",
    "bikes_df['total_rentals'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this tell us?\n",
    "\n",
    "There are more rentals in the winter than the spring, but only because the system is experiencing overall growth and the winter months happen to come after the spring months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the correlation matrix for the bikes `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:17:45.383311Z",
     "start_time": "2021-07-15T22:17:45.340898Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation matrix (ranges from 1 to -1)\n",
    "bikes_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a heat map to make it easier to read the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:18:15.331149Z",
     "start_time": "2021-07-15T22:18:14.715220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize correlation matrix in Seaborn using a heat map.\n",
    "sns.heatmap(bikes_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:15:34.694638Z",
     "start_time": "2020-12-07T23:15:34.168165Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.clustermap(bikes_df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What relationships do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T02:40:39.300574Z",
     "start_time": "2020-10-12T02:40:39.297748Z"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you not include `temp` and `atemp` together even though they are the closest related to `total_rentals`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"adding-more-features-to-the-model\"></a>\n",
    "### Adding More Features to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, one variable explained the variance of another; however, more often than not, we will need multiple variables. \n",
    "\n",
    "- For example, a house's price may be best measured by square feet, but a lot of other variables play a vital role: bedrooms, bathrooms, location, appliances, etc. \n",
    "\n",
    "- For a linear regression, we want these variables to be largely independent of one another, but all of them should help explain the y variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create another `LinearRegression` instance that is fit using temp, season, weather, and humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:21:10.726765Z",
     "start_time": "2021-07-15T22:21:10.703843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of features.\n",
    "feature_cols = ['temp', 'atemp','season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:21:20.314300Z",
     "start_time": "2021-07-15T22:21:20.263523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "X = bikes_df[feature_cols]\n",
    "y = bikes_df.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the linear regression coefficient along with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:21:54.920002Z",
     "start_time": "2021-07-15T22:21:54.903472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pair the feature names with the coefficients.\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "- Holding all other features fixed, a 1-unit increase in temperature is associated with a rental increase of 7.86 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in season is associated with a rental increase of 22.5 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in weather is associated with a rental increase of 6.67 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in humidity is associated with a rental decrease of 3.12 bikes.\n",
    "\n",
    "Does anything look incorrect and does not reflect reality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do X's significantly explain y? (i.e. do they have low P-Values).\n",
    "\n",
    "Unfortunately the Scikit learn linear regressor doesn't have a method to calculate p-values. There are a few ways we can solve for this one:\n",
    "1. Extend the linear regressor class (advanced)\n",
    "2. Run analysis against our dataset independently (statsmodel.api)\n",
    "3. Import another package to assist (regressors)\n",
    "4. Calculate the p-values with linear algebra (essentially what we'd automate in step 1)\n",
    "\n",
    "We could calculate these manually - but for efficiency we're going to use the statsmodels.api we imported as sm at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we haven't updated our linear regression model we could simply see the p-values from a standards OLS model from statsmodels.api which we imported as SM.**\n",
    "\n",
    "**Typically, the heuristics for significant p-values are either <.05 or <.10.**\n",
    "\n",
    "Statsmodels is a good method to analyze your data before you put it into your model. Once you understand the significance of different variables, you can build the final model using sklearn which includes more useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:26:22.050052Z",
     "start_time": "2021-07-15T22:26:21.778331Z"
    }
   },
   "outputs": [],
   "source": [
    "X2 = sm.add_constant(X)  #Adds our y intercept\n",
    "est = sm.OLS(y, X2)  # adds our OLS model\n",
    "est2 = est.fit() # fits our model\n",
    "print(est2.summary()) #Tada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-multicollinearity\"></a>\n",
    "## What Is Multicollinearity?\n",
    "---\n",
    "\n",
    "Multicollinearity happens when two or more features are highly correlated with each other. The problem is that due to the high correlation, it's hard to disambiguate which feature has what kind of effect on the outcome. In other words, the features mask each other. \n",
    "\n",
    "There is a second related issue called variance inflation where including correlated features increases the variability of our model and p-values by widening the standard errors. This can be measured with the variance inflation factor, which we will not cover here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a linear model that predicts `total_rentals` using `temp` and `atemp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:32:20.055360Z",
     "start_time": "2021-07-15T22:32:20.051647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of features.\n",
    "feature_cols = ['atemp'] #,'atemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:32:21.737931Z",
     "start_time": "2021-07-15T22:32:21.720707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "X = bikes_df[feature_cols]\n",
    "y = bikes_df.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(list(zip(feature_cols,linreg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back and remove either `temp` or `atemp` from the feature list. How do the coefficients change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T07:07:45.144793Z",
     "start_time": "2020-12-07T07:07:45.142276Z"
    }
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "#feature_cols = ['temp']\n",
    "#feature_cols = ['atemp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "#### 2. Las Vegas Multiple Variable Model \n",
    "Using the Las Vegas Trip Advisor data, build a model using 2 variables: `Hotel stars` and `Nr. reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:53:23.051898Z",
     "start_time": "2020-10-13T14:53:22.969353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of features.\n",
    "feature_cols = None\n",
    "\n",
    "# Create X and y.\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# create a linear regression\n",
    "\n",
    "# Print the coefficients & intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:53:24.449818Z",
     "start_time": "2020-10-13T14:53:24.331906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pair the feature names with the coefficients.\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your interpretations of the coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using statsmodels, interpret the coefficients?  Are any of them insignificant?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T14:53:43.211402Z",
     "start_time": "2020-10-13T14:53:43.107343Z"
    }
   },
   "outputs": [],
   "source": [
    "# use statsmodel to check the significance of the coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would be your next steps?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform, Score and compare RMSE.\n",
    "<a id=\"how-to-select-a-model\"></a>\n",
    "## How to Select a Model\n",
    "---\n",
    "\n",
    "We can make linear models now, but how do we select the best model to use for our applications? We will offer a general procedure and a simple metric that works well in many cases. That said, it's important to keep the business context in mind and know that there are alternative metrics that can work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-selection\"></a>\n",
    "### Feature Selection\n",
    "\n",
    "How do we choose which features to include in the model? We're going to use **train/test split** (and eventually **cross-validation**).\n",
    "\n",
    "Why not use p-values or R-squared for feature selection?\n",
    "\n",
    "- Linear models rely upon a lot of assumptions (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train/test split relies on fewer assumptions.\n",
    "- If all of the assumptions of a linear model are met, p-values suggest a coefficient that differs from zero at a level of statistical significance. This does not mean that\n",
    "    1. the feature _causes_ the response\n",
    "    2. the feature strongly _predicts_ the response. \n",
    "- Adding features to your model that are unrelated to the response will always increase the R-squared value, and adjusted R-squared does not sufficiently account for this (although, AIC and BIC do).\n",
    "- p-values and R-squared are **proxies** for our goal of generalization, whereas train/test split and cross-validation attempt to directly estimate how well the model will generalize to out-of-sample data.\n",
    "\n",
    "More generally:\n",
    "\n",
    "- There are different methodologies that can be used for solving any given data science problem, and this course follows a machine learning methodology.\n",
    "- This course focuses on general purpose approaches that can be applied to any model, rather than model-specific approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-metrics-for-regression-problems\"></a>\n",
    "### Evaluation Metrics for Regression Problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as accuracy, are not useful for regression problems. We need evaluation metrics designed for comparing continuous values.\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean absolute error (MAE)** is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean squared error (MSE)** is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root mean squared error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:38:56.858987Z",
     "start_time": "2021-07-15T22:38:56.802933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate MAE, MSE, and RMSE using imports from sklearn metrics and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:39:03.100492Z",
     "start_time": "2021-07-15T22:39:03.049551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate these metrics by hand!\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(f'MAE: {mean_absolute_error(true, pred)}')\n",
    "print(f'MSE:, {mean_squared_error(true, pred)}')\n",
    "print(f'RMSE:, {np.sqrt(mean_squared_error(true, pred))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these metrics:\n",
    "\n",
    "- MAE is the easiest to understand, because it's the average error.\n",
    "- MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world. Also, MSE is continuous and differentiable, making it easier to use than MAE for optimization.\n",
    "- RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "Here's an additional example, to demonstrate how MSE/RMSE punishes larger errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:41:27.874763Z",
     "start_time": "2021-07-15T22:41:27.866171Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same true values as above\n",
    "true = [10, 7, 5, 5]\n",
    "\n",
    "# New set of predicted values\n",
    "pred = [10, 7, 5, 13]\n",
    "\n",
    "# MAE is the same as before.\n",
    "print(f'MAE: {mean_absolute_error(true, pred)}')\n",
    "\n",
    "# MSE and RMSE are larger than before.\n",
    "print(f'MSE:, {mean_squared_error(true, pred)}')\n",
    "print(f'RMSE:, {np.sqrt(mean_squared_error(true, pred))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-models-with-traintest-split-and-rmse\"></a>\n",
    "### Comparing Models With Train/Test Split and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:44:22.361489Z",
     "start_time": "2021-07-15T22:44:22.328860Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a function that accepts a list of features and returns testing RMSE.\n",
    "def train_test_rmse(df, feature_cols, response):\n",
    "    \"\"\"accepts a list of features and returns testing RMSE\"\"\"\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    y = df[response]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = linreg.predict(X_test)\n",
    "    \n",
    "    return np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:45:34.591595Z",
     "start_time": "2021-07-15T22:45:34.474619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare different sets of features.\n",
    "\n",
    "feature_set_1 = train_test_rmse(bikes_df, ['temp', 'season', 'weather', 'humidity'], 'total_rentals')\n",
    "feature_set_2 = train_test_rmse(bikes_df, ['temp', 'season', 'weather'],'total_rentals')\n",
    "feature_set_3 = train_test_rmse(bikes_df, ['temp', 'season', 'humidity'],'total_rentals')\n",
    "\n",
    "print(f\"['temp', 'season', 'weather', 'humidity']: {feature_set_1}\")\n",
    "print(f\"            ['temp', 'season', 'weather']: {feature_set_2}\")\n",
    "print(f\"           ['temp', 'season', 'humidity']: {feature_set_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:47:17.871830Z",
     "start_time": "2021-07-15T22:47:17.818624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append scores to dataset\n",
    "X = bikes_df[['temp', 'season', 'humidity']]\n",
    "y = bikes_df['total_rentals']\n",
    "    \n",
    "# Split the data into training and testing data sets - \n",
    "# we use random_state to ensure our split is repeatable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "bikes_df['y_pred']= linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:47:36.369887Z",
     "start_time": "2021-07-15T22:47:36.299968Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What weird results do you notice?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:50:01.664663Z",
     "start_time": "2021-07-15T22:50:01.631396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using these as features is not allowed!\n",
    "casual_reg = train_test_rmse(bikes_df, ['casual', 'registered'],'total_rentals')\n",
    "print(f'RMSE for features casual & registered: {casual_reg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-testing-rmse-with-null-rmse\"></a>\n",
    "### Comparing Testing RMSE With Null RMSE\n",
    "\n",
    "Null RMSE is the RMSE that could be achieved by always predicting the mean response value. It is a benchmark against which you may want to measure your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:52:55.859479Z",
     "start_time": "2021-07-15T22:52:55.817670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "X = bikes_df['temp']\n",
    "y = bikes_df['total_rentals']\n",
    "\n",
    "# Split X and y into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "# Create a NumPy array with the same shape as y_test.\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# Fill the array with the mean value of y_test.\n",
    "y_null.fill(y_test.mean())\n",
    "y_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:54:19.297229Z",
     "start_time": "2021-07-15T22:54:19.290856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute null RMSE.\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_null))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will try both a square root and log transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:55:42.896937Z",
     "start_time": "2021-07-15T22:55:42.859474Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df['y_sqrt']= np.sqrt(y, dtype=float)\n",
    "bikes_df['y_cbrt']= np.cbrt(y, dtype=float)\n",
    "bikes_df['y_log']= np.log(y, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the distribution of the transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:55:45.306484Z",
     "start_time": "2021-07-15T22:55:44.720154Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df['y_sqrt'].plot();\n",
    "bikes_df['y_cbrt'].plot();\n",
    "bikes_df['y_log'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify our function to accept transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:56:02.424362Z",
     "start_time": "2021-07-15T22:56:02.399191Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_rmse_transform(df, feature_cols, response, transformation):\n",
    "    \"\"\"Compute RMSE for models of transformed targets \"\"\"\n",
    "    X = df[feature_cols]\n",
    "    y = df[response]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    \n",
    "    if transformation == 'sqrt':\n",
    "        y_pred_sqrt = linreg.predict(X_test)\n",
    "        y_pred = np.square(y_pred_sqrt)\n",
    "    elif transformation == 'log':\n",
    "        y_pred_log = linreg.predict(X_test)\n",
    "        y_pred = np.exp(y_pred_log)\n",
    "    elif transformation == 'cbrt':\n",
    "        y_pred_cbrt = linreg.predict(X_test)\n",
    "        y_pred = y_pred_cbrt**3\n",
    "    return np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the RMSE of the square root transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:56:38.692699Z",
     "start_time": "2021-07-15T22:56:38.613317Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_sqrt_1 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'weather', 'humidity'], 'y_sqrt', 'sqrt')\n",
    "rmse_sqrt_2 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'weather'],'y_sqrt', 'sqrt')\n",
    "rmse_sqrt_3 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'humidity'],'y_sqrt', 'sqrt')\n",
    "\n",
    "print(f\"['temp', 'season', 'weather', 'humidity'] RMSE: {rmse_sqrt_1}\")\n",
    "print(f\"            ['temp', 'season', 'weather'] RMSE: {rmse_sqrt_2}\")\n",
    "print(f\"           ['temp', 'season', 'humidity'] RMSE: {rmse_sqrt_3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the RMSE of the cube root transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:58:03.413593Z",
     "start_time": "2021-07-15T22:58:03.278299Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_cbrt_1 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'weather', 'humidity'], 'y_cbrt', 'cbrt')\n",
    "rmse_cbrt_2 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'weather'], 'y_cbrt', 'cbrt')\n",
    "rmse_cbrt_3 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'humidity'], 'y_cbrt', 'cbrt')\n",
    "\n",
    "print(f\"['temp', 'season', 'weather', 'humidity'] RMSE: {rmse_cbrt_1}\")\n",
    "print(f\"            ['temp', 'season', 'weather'] RMSE: {rmse_cbrt_2}\")\n",
    "print(f\"           ['temp', 'season', 'humidity'] RMSE: {rmse_cbrt_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the RMSE of the log transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:58:19.030402Z",
     "start_time": "2021-07-15T22:58:18.960284Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_log_1 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'weather', 'humidity'], 'y_log', 'log')\n",
    "rmse_log_2 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'weather'],'y_log', 'log')\n",
    "rmse_log_3 = train_test_rmse_transform(bikes_df, ['temp', 'season', 'humidity'],'y_log', 'log')\n",
    "\n",
    "print(f\"['temp', 'season', 'weather', 'humidity'] RMSE: {rmse_log_1}\")\n",
    "print(f\"            ['temp', 'season', 'weather'] RMSE: {rmse_log_2}\")\n",
    "print(f\"           ['temp', 'season', 'humidity'] RMSE: {rmse_log_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append the model scores to the original dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:58:50.406032Z",
     "start_time": "2021-07-15T22:58:50.383876Z"
    }
   },
   "outputs": [],
   "source": [
    "X = bikes_df[['temp', 'season', 'weather']]\n",
    "y = bikes_df['y_log']\n",
    "    \n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "# Instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:58:52.879577Z",
     "start_time": "2021-07-15T22:58:52.871174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "bikes_df['y_pred_log']= np.exp(linreg.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T22:58:53.928837Z",
     "start_time": "2021-07-15T22:58:53.856136Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "#### 3. Las Vegas Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a baseline/null model result to compare your models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "\n",
    "\n",
    "# Split X and y into training and testing sets.\n",
    "\n",
    "\n",
    "# Create a NumPy array with the same shape as y_test.\n",
    "\n",
    "\n",
    "# Fill the array with the mean value of y_test.\n",
    "\n",
    "\n",
    "# Compute null RMSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the single variable and multiple variable models using the `train_test_rmse` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which model is better?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the RMSE significantly lower than the RMSE in the bikes model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Square root transform the Score variable and compare the RMSE using the `train_test_rmse_transform` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE and compare to baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would you recommend?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering-to-improve-performance\"></a>\n",
    "## Feature Engineering to Improve Performance\n",
    "---\n",
    "\n",
    "Machine learning models are very powerful, but they cannot automatically handle every aspect of our data. We have to explicitly modify our features to have relationships that our models can understand. In this case, we will need to pull out features to have a linear relationship with our response variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"handling-categorical-features\"></a>\n",
    "### Handling Categorical Features\n",
    "\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "\n",
    "- **Ordered categories:** Transform them to sensible numeric values (example: small=1, medium=2, large=3)\n",
    "- **Unordered categories:** Use dummy encoding (0/1). Here, each possible category would become a separate feature.\n",
    "\n",
    "What are the categorical features in our data set?\n",
    "\n",
    "- **Ordered categories:** `weather` (already encoded with sensible numeric values)\n",
    "- **Unordered categories:** `season` (needs dummy encoding), `holiday` (already dummy encoded), `workingday` (already dummy encoded)\n",
    "\n",
    "For season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an ordered relationship. Instead, we create multiple dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummy variables using `get_dummies` from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:52:39.203910Z",
     "start_time": "2020-12-07T23:52:39.182823Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:52:34.547709Z",
     "start_time": "2020-12-07T23:52:34.536514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use get_dummies to transform categorical features into one-hot encoded data\n",
    "\n",
    "season_dummies = pd.get_dummies(bikes_df['season'], prefix='season')\n",
    "\n",
    "season_dummies.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the `DataFrame` of `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:55:01.396930Z",
     "start_time": "2020-12-07T23:55:01.366771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print five random rows.\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we actually only need three dummy variables (not four), and thus we'll drop the first dummy variable.\n",
    "\n",
    "Why? Because three dummies captures all of the \"information\" about the season feature, and implicitly defines spring (season 1) as the baseline level.\n",
    "\n",
    "This circles back to the concept multicollinearity, except instead of one feature being highly correlated to another, the information gained from three features is directly correlated to the fourth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:57:34.722748Z",
     "start_time": "2020-12-07T23:57:34.710296Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the first column\n",
    "#season_dummies.drop(season_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Redo the get_dummies command and use 'drop_first' to drop the first dummy column.\n",
    "season_dummies = pd.get_dummies(bikes_df['season'], prefix='season', drop_first=True)\n",
    "\n",
    "season_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinspect the `DataFrame` of `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:58:13.823160Z",
     "start_time": "2020-12-07T23:58:13.802788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print five random rows.\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if you have a categorical feature with k possible values, you create k-1 dummy variables.\n",
    "\n",
    "If that's confusing, think about why we only need one dummy variable for `holiday`, not two dummy variables (`holiday_yes` and `holiday_no`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now need to concatenate the two `DataFrames` together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:58:41.940656Z",
     "start_time": "2020-12-07T23:58:41.911888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns).\n",
    "bikes_dummies = pd.concat([bikes_df, season_dummies], axis=1)\n",
    "\n",
    "# Print 5 random rows.\n",
    "bikes_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun the linear regression with dummy variables included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T23:58:53.369789Z",
     "start_time": "2020-12-07T23:58:53.356624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Include dummy variables for season in the model.\n",
    "feature_cols = ['temp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "X = bikes_dummies[feature_cols]\n",
    "y = bikes_dummies.total_rentals\n",
    "\n",
    "# Instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# display the coefficients along with their names\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the season coefficients? They are measured against the baseline (spring):\n",
    "\n",
    "- Holding all other features fixed, summer is associated with a rental decrease of 3.39 bikes compared to the spring.\n",
    "- Holding all other features fixed, fall is associated with a rental decrease of 41.7 bikes compared to the spring.\n",
    "- Holding all other features fixed, winter is associated with a rental increase of 64.4 bikes compared to the spring.\n",
    "\n",
    "Would it matter if we changed which season was defined as the baseline?\n",
    "\n",
    "- No, it would simply change our interpretation of the coefficients.\n",
    "\n",
    "In most situations, it is best to have the dummy that is your baseline be the category that has the largest representation.\n",
    "\n",
    "**Important:** Dummy encoding is relevant for all machine learning models, not just linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-13T15:54:11.408790Z",
     "start_time": "2020-10-13T15:54:11.342022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare original season variable with dummy variables.\n",
    "rmse_dummy_1 = train_test_rmse(bikes_dummies, ['temp', 'season', 'humidity'],'total_rentals')\n",
    "rmse_dummy_2 = train_test_rmse(bikes_dummies, ['temp', 'season_2', 'season_3', 'season_4', 'humidity'],'total_rentals')\n",
    "\n",
    "print(f\"['temp', 'season', 'humidity'] RMSE: {rmse_dummy_1}\")\n",
    "print(f\"['temp', 'season_2', 'season_3', 'season_4', 'humidity'] RMSE: {rmse_dummy_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "#### 4. Bike Weather Dummy Variables\n",
    "Build dummy variables for weather, append it to the `bike_dummies` dataframe, and check the model performance with both `total rentals` and the log transformation, using `temp`, `season` and `weather`. Use the `drop_first=True` parameter in `pd.get_dummies`.\n",
    "\n",
    "Once complete, check the results of the log transformation just using `temp` as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transform y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare RMSE with dummy variables and temp only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp only model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are your conclusions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "### More Feature Engineering\n",
    "\n",
    "See if you can create the following features:\n",
    "\n",
    "- **hour:** as a single numeric feature (0 through 23)\n",
    "- **hour:** as a categorical feature (use 23 dummy variables)\n",
    "- **daytime:** as a single categorical feature (daytime=1 from 7 a.m. to 8 p.m., and daytime=0 otherwise)\n",
    "\n",
    "Then, try using each of the three features (on its own) with `train_test_rmse` to see which one performs the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract hour of the day to use as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:28:49.947836Z",
     "start_time": "2020-10-12T04:28:49.927938Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df['hour'] = bikes_df.index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode `hour` as a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:29:05.210190Z",
     "start_time": "2020-10-12T04:29:05.188929Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode the hour feature\n",
    "hour_dummies = pd.get_dummies(bikes_df.hour, prefix='hour',drop_first=True)\n",
    "bikes_df = pd.concat([bikes_df, hour_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a `daytime` variable based on hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:29:27.564144Z",
     "start_time": "2020-10-12T04:29:27.558050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tip - use conditions to set a value that identifies day vs night (think sunrise to sunset?)\n",
    "bikes_df['daytime'] = ((bikes_df.hour > 6) & (bikes_df.hour < 19)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:30:13.562782Z",
     "start_time": "2020-10-12T04:30:13.540080Z"
    }
   },
   "outputs": [],
   "source": [
    "bikes_df.head(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the root mean squared error of our various `hour` encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:30:27.383021Z",
     "start_time": "2020-10-12T04:30:27.325468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remember we built the function train_test_rmse()\n",
    "rmse_hour = train_test_rmse(bikes_df, ['hour'], ['total_rentals'])\n",
    "rmse_hourly = train_test_rmse(bikes_df, bikes_df.columns[bikes_df.columns.str.startswith('hour_')],['total_rentals'])\n",
    "rmse_daytime = train_test_rmse(bikes_df, ['daytime'],['total_rentals'])\n",
    "\n",
    "print(f'Hourly RMSE: {rmse_hour}')\n",
    "print(f'Hourly RMSE: {rmse_hourly}')\n",
    "print(f'Daytime RMSE: {rmse_daytime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bike Share Bonus: Building models for other y variables\n",
    "\n",
    "We've completely a model together that explains total rides. Now it's your turn to build another model, using a different y variable: registered riders.\n",
    "\n",
    "#### Pay attention to:\n",
    "\n",
    "* the distribution of riders (should we rescale the data?)  \n",
    "* checking correlations with variables and registered riders  \n",
    "* having a feature space (our matrix) with low multicollinearity  \n",
    "* model complexity vs explanation of variance: at what point do features in a model stop improving r-squared?  \n",
    "* the linear assumption -- given all feature values being 0, should we have no ridership? negative ridership? positive ridership?\n",
    "\n",
    "#### Bonus\n",
    "\n",
    "* Which variables would make sense to dummy (because they are categorical, not continuous)?  \n",
    "* What features might explain ridership but aren't included in the data set? \n",
    "* Is there a way to build these using pandas and the features available?\n",
    "* Outcomes If your model at least improves upon the original model and the explanatory effects (coefficients) make sense, consider this a complete task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "#### 5. Las Vegas Dummy Coding\n",
    "Add `Traveler type` and `Pool` variables to `Hotel stars` and model to predict `Score`.  Remember to use your best transformation from the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "\n",
    "\n",
    "# Concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have the hypothesis that your model may be different by season.  What if you built a model for just Mar-May period of stay? Compare using just `Hotel stars` vs. `Hotel stars` and the traveler type dummy variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset Mar-May Period of stay and use in the train_test_rmse_transform function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you compare the RMSE between the seasonal model vs. the entire dataset? What is your conclusion about all of the model results?  What are your next steps?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bonus-material-regularization\"></a>\n",
    "## Bonus Material: Regularization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bonus-material-regularization\"></a>\n",
    "## Bonus Material: Regularization\n",
    "---- Regularization is a method for \"constraining\" or \"regularizing\" the size of the coefficients, thus \"shrinking\" them toward zero.\n",
    "- It reduces model variance and thus minimizes overfitting.\n",
    "- If the model is too complex, it tends to reduce variance more than it increases bias, resulting in a model that is more likely to generalize.\n",
    "\n",
    "Our goal is to locate the optimum model complexity, and thus regularization is useful when we believe our model is too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-does-regularization-work\"></a>\n",
    "### How Does Regularization Work?\n",
    "\n",
    "For a normal linear regression model, we estimate the coefficients using the least squares criterion, which minimizes the residual sum of squares (RSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a regularized linear regression model, we minimize the sum of RSS and a \"penalty term\" that penalizes coefficient size.\n",
    "\n",
    "**Ridge regression** (or \"L2 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Lasso regression** (or \"L1 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "- $p$ is the number of features.\n",
    "- $\\beta_j$ is a model coefficient.\n",
    "- $\\alpha$ is a tuning parameter:\n",
    "    - A tiny $\\alpha$ imposes no penalty on the coefficient size, and is equivalent to a normal linear regression model.\n",
    "    - Increasing the $\\alpha$ penalizes the coefficients and thus shrinks them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lasso-and-ridge-path-diagrams\"></a>\n",
    "### Lasso and Ridge Path Diagrams\n",
    "\n",
    "A larger alpha (toward the left of each diagram) results in more regularization:\n",
    "\n",
    "- Lasso regression shrinks coefficients all the way to zero, thus removing them from the model.\n",
    "- Ridge regression shrinks coefficients toward zero, but they rarely reach zero.\n",
    "\n",
    "Source code for the diagrams: [Lasso regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html) and [Ridge regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lasso and Ridge Coefficient Plots](./assets/lasso_ridge_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advice-for-applying-regularization\"></a>\n",
    "### Advice for Applying Regularization\n",
    "\n",
    "**Should features be standardized?**\n",
    "\n",
    "- Yes, because otherwise, features would be penalized simply because of their scale.\n",
    "- Also, standardizing avoids penalizing the intercept, which wouldn't make intuitive sense.\n",
    "\n",
    "**How should you choose between lasso regression and ridge regression?**\n",
    "\n",
    "- Lasso regression is preferred if we believe many features are irrelevant or if we prefer a sparse model.\n",
    "- Ridge can work particularly well if there is a high degree of multicollinearity in your model.\n",
    "- If model performance is your primary concern, it is best to try both.\n",
    "- Elastic net regression is a combination of lasso regression and ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ridge-regression\"></a>\n",
    "### Ridge Regression\n",
    "\n",
    "- [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) documentation\n",
    "- **alpha:** must be positive, increase for more regularization\n",
    "- **normalize:** scales the features (without using StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:45:38.347678Z",
     "start_time": "2020-10-12T04:45:38.334270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Include dummy variables for season in the model.\n",
    "feature_cols = ['temp', 'atemp', 'season_2', 'season_3', 'season_4', 'humidity']\n",
    "\n",
    "X = bikes_dummies[feature_cols]\n",
    "y = bikes_dummies.total_rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:45:39.231043Z",
     "start_time": "2020-10-12T04:45:39.222629Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:45:45.250341Z",
     "start_time": "2020-10-12T04:45:45.234727Z"
    }
   },
   "outputs": [],
   "source": [
    "# alpha=0 is equivalent to linear regression.\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Instantiate the model.\n",
    "#(Alpha of zero has no regularization strength, essentially a basic linear regression.)\n",
    "ridgereg = Ridge(alpha=0, normalize=True)\n",
    "\n",
    "# Fit the model.\n",
    "ridgereg.fit(X_train, y_train)\n",
    "\n",
    "# Predict with fitted model.\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:45:46.263175Z",
     "start_time": "2020-10-12T04:45:46.256871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coefficients for a non-regularized linear regression\n",
    "list(zip(feature_cols, ridgereg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret these coefficients we need to convert them back to original units, which is a reason to do normalization by hand. However, in this form the coefficients have a special meaning. The intercept is now the average of our outcome, and the magnitude of each coefficient in the model is a measure of how important it is in the model. We call this feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T04:45:52.711690Z",
     "start_time": "2020-10-12T04:45:52.698437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try alpha=0.1.\n",
    "ridgereg = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the coefficients.\n",
    "list(zip(feature_cols, ridgereg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the MSE barely changed, we can see there are significant changes in the weight of our coefficients.  Particularly `season_2` whose coefficient has greatly decreased toward 0.\n",
    "\n",
    "Fitting and using a Lasso Regression in scikit-learn is very similar.  \n",
    "\n",
    "In addition to the typical [lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and [ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) there is a third type of regression, [Elastic Net](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) which combines the penalties of the ridge and lasso methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-linear-regression-with-other-models\"></a>\n",
    "## Comparing Linear Regression With Other Models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain.\n",
    "- Highly interpretable.\n",
    "- Model training and prediction are fast.\n",
    "- No tuning is required (excluding regularization).\n",
    "- Features don't need scaling.\n",
    "- Can perform well with a small number of observations.\n",
    "- Well understood.\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response.\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias.\n",
    "- Can't automatically learn feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
